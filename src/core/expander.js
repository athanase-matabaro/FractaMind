/**
 * Expansion Layer for FractaMind
 *
 * Handles node expansion using Chrome Writer API to generate child nodes,
 * compute embeddings, Morton keys, and persist to IndexedDB.
 *
 * Features:
 * - Deduplication by content hash
 * - Rate limiting with exponential backoff
 * - Automatic parent update
 */

import { expandNode as chromeExpandNode, generateEmbedding } from '../ai/chromeAI.js';
import {
  getNode,
  saveNode,
  computeQuantizationParams,
  computeMortonKeyFromEmbedding,
} from '../db/fractamind-indexer.js';
import { generateUUID } from '../utils/uuid.js';

// Rate limiting state
let lastExpandTime = 0;
const MIN_EXPAND_INTERVAL_MS = 1000; // Minimum 1 second between expansions

// Content hash cache for deduplication
const contentHashCache = new Map();

/**
 * Simple hash function for content deduplication
 */
function hashContent(text) {
  let hash = 0;
  for (let i = 0; i < text.length; i++) {
    const char = text.charCodeAt(i);
    hash = (hash << 5) - hash + char;
    hash = hash & hash; // Convert to 32-bit integer
  }
  return hash.toString(36);
}

/**
 * Rate limit check with exponential backoff
 */
async function waitForRateLimit(attemptCount = 0) {
  const now = Date.now();
  const timeSinceLastExpand = now - lastExpandTime;

  if (timeSinceLastExpand < MIN_EXPAND_INTERVAL_MS) {
    const waitTime = MIN_EXPAND_INTERVAL_MS - timeSinceLastExpand;
    const backoffTime = waitTime * Math.pow(1.5, attemptCount);
    await new Promise((resolve) => setTimeout(resolve, backoffTime));
  }

  lastExpandTime = Date.now();
}

/**
 * Expand a node into child nodes using Writer API
 *
 * @param {string} nodeId - ID of node to expand
 * @param {Object} options - Expansion options
 * @param {number} options.depth - Expansion depth (default: 1)
 * @param {string} options.style - Expansion style: 'concise' | 'detailed' (default: 'concise')
 * @param {number} options.maxChildren - Maximum children to generate (default: 3)
 * @param {Function} options.onProgress - Progress callback (optional)
 * @returns {Promise<Array<Object>>} - Array of created child nodes
 */
export async function expandNode(nodeId, options = {}) {
  const {
    depth = 1,
    style = 'concise',
    maxChildren = 3,
    onProgress = null,
    attemptCount = 0,
  } = options;

  try {
    // Rate limiting
    await waitForRateLimit(attemptCount);

    onProgress?.({ step: 'loading', progress: 0.1, message: 'Loading parent node...' });

    // 1. Fetch parent node
    const parentNode = await getNode(nodeId);
    if (!parentNode) {
      throw new Error(`Node ${nodeId} not found`);
    }

    onProgress?.({ step: 'expanding', progress: 0.2, message: 'Generating child ideas...' });

    // 2. Call Writer API to expand node
    const aiResponse = await chromeExpandNode(parentNode.text, {
      title: parentNode.title,
      numChildren: maxChildren,
      style,
    });

    if (!aiResponse || aiResponse.length === 0) {
      throw new Error('No child nodes generated by AI');
    }

    onProgress?.({ step: 'deduplicating', progress: 0.4, message: 'Checking for duplicates...' });

    // 3. Deduplicate child nodes
    const existingChildren = parentNode.children || [];
    const existingChildNodes = await Promise.all(
      existingChildren.map((id) => getNode(id).catch(() => null))
    );

    const existingHashes = new Set(
      existingChildNodes.filter(Boolean).map((node) => hashContent(node.text))
    );

    const uniqueChildren = aiResponse.filter((child) => {
      const hash = hashContent(child.text);
      if (existingHashes.has(hash)) {
        console.log('Skipping duplicate child:', child.title);
        return false;
      }
      existingHashes.add(hash);
      return true;
    });

    if (uniqueChildren.length === 0) {
      console.warn('All generated children were duplicates');
      return [];
    }

    onProgress?.({
      step: 'embedding',
      progress: 0.5,
      message: `Generating embeddings for ${uniqueChildren.length} nodes...`,
    });

    // 4. Generate embeddings for all children
    const childTexts = uniqueChildren.map((child) => `${child.title}. ${child.text}`);
    const embeddings = await Promise.all(
      childTexts.map((text) => generateEmbedding(text.slice(0, 2000)))
    );

    // 5. Compute quantization parameters
    const embeddingArrays = embeddings.map((e) => Array.from(e));
    const quantParams = computeQuantizationParams(embeddingArrays, {
      reducedDims: 8,
      bits: 16,
      reduction: 'first',
    });

    onProgress?.({ step: 'persisting', progress: 0.7, message: 'Saving child nodes...' });

    // 6. Create and persist child nodes
    const childNodes = [];
    const newChildIds = [];

    for (let i = 0; i < uniqueChildren.length; i++) {
      const child = uniqueChildren[i];
      const embedding = embeddingArrays[i];

      // Compute Morton key
      const mortonKeyHex = computeMortonKeyFromEmbedding(embedding, quantParams);

      // Create child node
      const childNode = {
        id: generateUUID(),
        title: child.title || `Untitled Child ${i + 1}`,
        text: child.text || '',
        summary: child.text?.slice(0, 150) || '',
        children: [],
        parent: nodeId,
        embedding,
        hilbertKeyHex: mortonKeyHex,
        meta: {
          projectId: parentNode.meta?.projectId || null,
          createdAt: new Date().toISOString(),
          createdBy: 'expander',
          depth: (parentNode.meta?.depth || 0) + depth,
          expandStyle: style,
        },
      };

      // Persist child node
      await saveNode(childNode);
      childNodes.push(childNode);
      newChildIds.push(childNode.id);

      // Update content hash cache
      contentHashCache.set(childNode.id, hashContent(childNode.text));
    }

    onProgress?.({ step: 'updating-parent', progress: 0.9, message: 'Updating parent node...' });

    // 7. Update parent node's children list
    const updatedParent = {
      ...parentNode,
      children: [...(parentNode.children || []), ...newChildIds],
      meta: {
        ...parentNode.meta,
        updatedAt: new Date().toISOString(),
        expandCount: (parentNode.meta?.expandCount || 0) + 1,
      },
    };

    await saveNode(updatedParent);

    onProgress?.({ step: 'complete', progress: 1.0, message: 'Expansion complete!' });

    console.log(`Expanded node ${nodeId}: created ${childNodes.length} children`);
    return childNodes;
  } catch (error) {
    console.error('Expansion failed:', error);

    // Retry with exponential backoff on rate limit errors
    if (
      error.message.includes('rate limit') ||
      error.message.includes('too many requests')
    ) {
      if (attemptCount < 3) {
        console.log(`Retrying expansion (attempt ${attemptCount + 1}/3)...`);
        return expandNode(nodeId, { ...options, attemptCount: attemptCount + 1 });
      }
    }

    throw new Error(`Expansion failed: ${error.message}`);
  }
}

/**
 * Batch expand multiple nodes
 *
 * @param {string[]} nodeIds - Array of node IDs to expand
 * @param {Object} options - Expansion options (same as expandNode)
 * @returns {Promise<Map<string, Array<Object>>>} - Map of nodeId -> child nodes
 */
export async function batchExpandNodes(nodeIds, options = {}) {
  const results = new Map();

  for (const nodeId of nodeIds) {
    try {
      const childNodes = await expandNode(nodeId, options);
      results.set(nodeId, childNodes);
    } catch (error) {
      console.error(`Failed to expand node ${nodeId}:`, error);
      results.set(nodeId, []);
    }
  }

  return results;
}

/**
 * Get expansion history for a node
 *
 * @param {string} nodeId - Node ID
 * @returns {Promise<Object>} - Expansion metadata
 */
export async function getExpansionHistory(nodeId) {
  const node = await getNode(nodeId);

  if (!node) {
    throw new Error(`Node ${nodeId} not found`);
  }

  const children = await Promise.all(
    (node.children || []).map((id) => getNode(id).catch(() => null))
  );

  return {
    nodeId,
    title: node.title,
    childCount: children.filter(Boolean).length,
    expandCount: node.meta?.expandCount || 0,
    lastExpanded: node.meta?.updatedAt || node.meta?.createdAt,
    children: children.filter(Boolean).map((child) => ({
      id: child.id,
      title: child.title,
      createdAt: child.meta?.createdAt,
      createdBy: child.meta?.createdBy,
    })),
  };
}

/**
 * Clear content hash cache (useful for testing)
 */
export function clearContentHashCache() {
  contentHashCache.clear();
}
